# üöõ LogiCash: Optimizaci√≥n Log√≠stica de Efectivo

**Enfoque:** Data Engineering Batch, Hybrid Cloud (AWS & Local) & FinOps.

## üìã Caso de Negocio
La "Financiera LogiCash" busca optimizar el reabastecimiento de sus cajeros autom√°ticos (ATMs). Actualmente sufren de:
1.  **Sobrecostos:** Camiones visitando oficinas que no necesitan dinero.
2.  **Cash-out:** Oficinas qued√°ndose sin efectivo en d√≠as cr√≠ticos.

Este proyecto migra la inteligencia log√≠stica a una arquitectura de Big Data en AWS.

## üèó Arquitectura T√©cnica
* **Lenguaje:** Python (PySpark).
* **Infraestructura Local:** Docker (simulando AWS Glue).
* **Infraestructura Nube:** AWS Glue, S3, Redshift Serverless.
* **Orquestaci√≥n:** Event-driven (S3 Events).

## üöÄ Quick Start (Entorno Local)

### 1. Generaci√≥n de Data (Mock)
Genera datos de prueba con errores intencionales (data sucia) para probar el pipeline.
```bash
python src/data_gen/data_generator.py
```

### 2. Levantar Entorno Docker (AWS Glue Local)
Este proyecto usa la imagen oficial de AWS Glue para desarrollo local costo cero.

**Paso 1: Descargar Imagen**
```bash
docker pull amazon/aws-glue-libs:glue_libs_4.0.0_image_01
```

**Paso 2: Iniciar Contenedor**

En Windows (PowerShell):
```powershell
docker run -it -v ${PWD}:/home/glue_user/workspace/ -p 8888:8888 -p 4040:4040 --name logicash_glue amazon/aws-glue-libs:glue_libs_4.0.0_image_01
```

En Mac/Linux:
```bash
docker run -it -v $(pwd):/home/glue_user/workspace/ -p 8888:8888 -p 4040:4040 --name logicash_glue amazon/aws-glue-libs:glue_libs_4.0.0_image_01
```

**Paso 3: Comandos √ötiles**
```bash
# Volver a iniciar (si reinicias la PC)
docker start -ai logicash_glue

# Abrir Spark Shell (dentro del contenedor)
pyspark
```

---

### 3. Retomando: Tu "Hola Mundo" en Spark ‚ö°

Una vez que guardes ese `README.md` (y si quieres haz un commit/push para asegurar), volvamos a la terminal negra donde eres el usuario `glue_user`.

**Tu Misi√≥n:**
1.  Aseg√∫rate de estar dentro del contenedor (el prompt debe decir `bash-4.2$` o similar, no tu ruta de Windows).
2.  Escribe el comando:
    ```bash
    pyspark
    ```
3.  Espera unos segundos. Ver√°s muchos mensajes de carga (INFO).
4.  Si todo sale bien, ver√°s un arte ASCII gigante que dice **Spark**.

**Cuando veas ese logo de Spark, c√≥pialo y p√©gamelo aqu√≠ (o conf√≠rmame).** ¬°Ese es el momento en que oficialmente tienes un cluster de Big Data corriendo en tu laptop!
